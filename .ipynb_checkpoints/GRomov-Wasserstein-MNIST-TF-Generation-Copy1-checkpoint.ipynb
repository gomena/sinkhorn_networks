{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gonzalomena/anaconda3/envs/python2/lib/python2.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-bcf54faddac5>:15: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/gonzalomena/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/gonzalomena/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/gonzalomena/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/gonzalomena/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/gonzalomena/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Extracting /tmp/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import optimizer\n",
    "import sinkhorn_ops\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "%matplotlib inline\n",
    "from time import time\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data_train = input_data.read_data_sets('/tmp/', one_hot=True).train \n",
    "data_test = input_data.read_data_sets('/tmp/', one_hot=True).test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "def sinkhorn(log_alpha, n_iters=20):\n",
    "\n",
    "    for n in range(n_iters):\n",
    "        log_alpha -= logsumexp(log_alpha, axis=1, keepdims=True)\n",
    "        log_alpha -= logsumexp(log_alpha, axis=0, keepdims=True)\n",
    "        if n%10000==1:\n",
    "            print(n)\n",
    "    log_alpha -= logsumexp(log_alpha, axis=1, keepdims=True)\n",
    "    return np.exp(log_alpha)\n",
    "\n",
    "def batch_split(batch, n_squares_side, n_channels=1):\n",
    "    if(n_channels ==1):\n",
    "        #side = int(np.sqrt(batch.shape[1]))\n",
    "        side =batch.shape[1]\n",
    "    else:\n",
    "        side = batch.shape[1]\n",
    "    batch_size = batch.shape[0]\n",
    "    n_squares = n_squares_side ** 2\n",
    "    \n",
    "    batch = np.reshape(batch, [-1, side, side, n_channels])\n",
    "    batch = np.reshape(batch, [batch_size, n_squares_side, side/n_squares_side, side, n_channels])\n",
    "    batch = np.transpose(batch, [0, 2, 1, 3, 4])\n",
    "    batch = np.reshape(batch, [batch_size, side/n_squares_side, n_squares, side/n_squares_side, n_channels])\n",
    "    batch = np.transpose(batch, [0, 2, 1, 3, 4])\n",
    "    return batch\n",
    "\n",
    "def stack_batch_split(batch):\n",
    "    return np.reshape(batch, [batch.shape[0]*batch.shape[1], batch.shape[2], batch.shape[3], batch.shape[4]])\n",
    "\n",
    "\n",
    "def unflatten_batch(batch, n_channels=1):\n",
    "    side_square = int(np.sqrt(batch.shape[2]/n_channels))\n",
    "    return np.reshape(batch, [batch.shape[0], batch.shape[1], side_square, side_square, n_channels])\n",
    "\n",
    "def join_batch_split(batch):\n",
    "    batch_size = batch.shape[0]\n",
    "    n_squares = batch.shape[1]\n",
    "    side_quare = batch.shape[2]\n",
    "    n_channels = batch.shape[4]\n",
    "    n_squares_side = int(np.sqrt(n_squares))\n",
    "    batch = np.transpose(batch, [0, 1, 3, 2, 4])\n",
    "    batch = np.reshape(batch, [batch_size, n_squares_side, side_square*n_squares_side, side_square, n_channels])\n",
    "    batch = np.transpose(batch, [0,1, 3,2,4])\n",
    "    batch = np.reshape(batch, [batch_size, 1, side_square*n_squares_side, side_square*n_squares_side, n_channels])\n",
    "    batch = np.reshape(batch, [batch_size, side_square*n_squares_side, side_square*n_squares_side, n_channels])\n",
    "    return batch\n",
    "\n",
    "def resized_dims(n_squares_side):\n",
    "    if(n_squares_side==2):\n",
    "        side = 28\n",
    "        side_square = 14\n",
    "    if(n_squares_side==3):\n",
    "        side = 27\n",
    "        side_square = 9\n",
    "    if(n_squares_side==4):\n",
    "        side = 28\n",
    "        side_square = 7\n",
    "    if(n_squares_side==5):\n",
    "        side = 30\n",
    "        side_square = 6\n",
    "    if(n_squares_side==6):\n",
    "        side = 30\n",
    "        side_square = 5\n",
    "    if(n_squares_side==7):\n",
    "        side = 28\n",
    "        side_square = 4\n",
    "    if(n_squares_side==8):\n",
    "        side = 32\n",
    "        side_square = 4\n",
    "    if(n_squares_side==9):\n",
    "        side = 27\n",
    "        side_square = 3\n",
    "    if(n_squares_side==16):\n",
    "        side = 80\n",
    "        side_square = 5\n",
    "    if(n_squares_side==14):\n",
    "        side = 196\n",
    "        side_square = 14\n",
    "    if(n_squares_side==18):\n",
    "        side = 8*18\n",
    "        side_square = 8\n",
    "    if(n_squares_side==20):\n",
    "        side = 7*20\n",
    "        side_square = 7\n",
    "    if(n_squares_side==25):\n",
    "        side = 25*6\n",
    "        side_square = 6\n",
    "    if(n_squares_side==30):\n",
    "        side = 30*5\n",
    "        side_square = 5\n",
    "    if (n_squares_side == 37):\n",
    "        side = 37 * 4\n",
    "        side_square = 4\n",
    "    if (n_squares_side == 45):\n",
    "        side = 45 * 3\n",
    "        side_square = 3\n",
    "    return side, side_square\n",
    "\n",
    "def resize_batch_color(batch, side_new, n_channels):\n",
    "    batch_new = np.zeros((batch.shape[0], side_new, side_new, n_channels))\n",
    "    side = int(np.sqrt(batch.shape[1]))\n",
    "    for i in range(batch.shape[0]):\n",
    "        for c in range(n_channels):\n",
    "            a = imresize(batch[i,:,:,c], [side_new, side_new])\n",
    "            \n",
    "            a = a/255.0\n",
    "            batch_new[i,:,:,c] =a\n",
    "    return batch_new\n",
    "\n",
    "\n",
    "def soft_to_hard(soft_perm):\n",
    "    \n",
    "    a,b = linear_sum_assignment(-soft_perm)\n",
    "    ma = np.zeros((np.shape(soft_perm)))\n",
    "    for i in range(soft_perm.shape[0]):\n",
    "        ma[i, b[i]] = 1\n",
    "    return ma\n",
    "\n",
    "\n",
    "def load_image( infilename ) :\n",
    "    img = Image.open( infilename )\n",
    "    img.load()\n",
    "    data = np.asarray( img, dtype=\"int32\" )\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_z():\n",
    "    #create the matrix of log_alpha, that will later will converted into a soft permutation\n",
    "    #this relies on some NN processing (convolutional), see below\n",
    "    fc = tf.contrib.layers.fully_connected\n",
    "    flatten = tf.contrib.layers.flatten\n",
    "    dropout = tf.contrib.layers.dropout\n",
    "    def conv(input_image, kernel_shape, bias_shape):\n",
    "        weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "                                 initializer = tf.random_normal_initializer())\n",
    "        biases = tf.get_variable(\"biases\", bias_shape, \n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "        convolutional = tf.nn.conv2d(input_image, weights, \n",
    "                                     strides = [1, 1, 1, 1],\n",
    "                                     padding=\"SAME\")\n",
    "        out_relu = tf.nn.relu(convolutional + biases)\n",
    "        out_maxpool = tf.nn.max_pool(out_relu, \n",
    "                                    ksize=[1, stride, stride, 1],\n",
    "                                   strides=[1, stride, stride, 1],\n",
    "                                   padding=\"SAME\")\n",
    "        return out_maxpool\n",
    "    def conv_and_fc(input_image, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            conv_output = conv(input_image, [rfield_size, rfield_size, n_channels, n_units], [n_units])\n",
    "        fully_connected_output_mean = dropout(tf.cast(fc(flatten(conv_output), n_dim_z, activation_fn = None), tf.float32),\n",
    "                                        keep_prob)\n",
    "        fully_connected_output_log_var = dropout(tf.cast(fc(flatten(conv_output), n_dim_z, activation_fn = None), tf.float32),\n",
    "                                        keep_prob)\n",
    "      \n",
    "        return fully_connected_output_mean, fully_connected_output_log_var\n",
    "    \n",
    "    #with tf.variable_scope(\"model_params\"):\n",
    "    z_mean, z_log_var = conv_and_fc(real_entire_tiled,\"conv_1\")\n",
    "    z  = z_mean + tf.random.normal([batch_size, n_dim_z]) *tf.exp(z_log_var/2)\n",
    "    z1 = tf.reshape(fc(z, n_squares*n_dim_z2), [-1, n_squares, n_dim_z2])\n",
    "    #z2 = tf.reshape(fc(z, n_squares*n_dim_z2), [-1, n_dim_z2, n_squares])\n",
    "    cost = tf.nn.sigmoid(tf.matmul(z1,tf.transpose(z1,[0,2,1])))\n",
    "    #cost = tf.nn.sigmoid(tf.reshape(fc(z, n_squares*n_squares), [-1, n_squares,  n_squares]))\n",
    "\n",
    "    return z,z_mean, z_log_var, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# Define model params\n",
    "batch_size = 100\n",
    "n_iter_sinkhorn2 = 10\n",
    "n_iter_sinkhorn = 200\n",
    "\n",
    "temp = 2.0\n",
    "temp2 =0.1\n",
    "\n",
    "#mnist data\n",
    "n_squares_side = 25\n",
    "lr = 0.05\n",
    "n_channels = 1\n",
    "rfield_size = 3\n",
    "stride = 3\n",
    "n_units = 2\n",
    "keep_prob = 1.0\n",
    "side_real = 28\n",
    "opt = 'adam'\n",
    "samples_per_num = 1\n",
    "n_squares = n_squares_side **2\n",
    "n_gromov = 5\n",
    "side, side_square = resized_dims(n_squares_side)\n",
    "n_dim = int(side_square*side_square*n_channels)\n",
    "print(n_dim)\n",
    "n_dim_z = 5\n",
    "n_dim_z2 = 5\n",
    "\n",
    "noise_factor = 0\n",
    "np.random.seed(2)\n",
    "\n",
    "ims0,_=data_train.next_batch(1)\n",
    "ims0 = np.expand_dims(np.reshape(ims0, [-1, side_real,side_real]),axis=3)\n",
    "ims0[ims0>0.5]=1\n",
    "ims0[ims0<0.5]=0\n",
    "\n",
    "\n",
    "\n",
    "nx = np.nansum(ims0) \n",
    "prop = nx/(side_real**2)*np.random.uniform(1.5,2.0)\n",
    "pieces_split = np.zeros((batch_size, n_squares, side_square, side_square, 1))\n",
    "for j in range(int(n_squares*prop)):\n",
    "    pieces_split[:,j,:,:] = 1\n",
    "\n",
    "scrambled_pieces_split = np.zeros(pieces_split.shape)\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "perm = np.random.permutation(n_squares)\n",
    "\n",
    "scrambled_pieces_split[:,:, :, :] = pieces_split[:, perm, :, :]\n",
    "stacked_scrambled_pieces_split = stack_batch_split(scrambled_pieces_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19335011239180147\n"
     ]
    }
   ],
   "source": [
    "print(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define the main TF variables\n",
    "\n",
    "scrambled_split = tf.placeholder(tf.float32,[None, n_squares, side_square, side_square, n_channels])\n",
    "scrambled_split_tiled = tf.tile(scrambled_split, [samples_per_num, 1, 1, 1, 1])\n",
    "\n",
    "stack_scrambled_pieces_split = tf.placeholder(tf.float32,[None, side_square, side_square, n_channels])\n",
    "\n",
    "real_split = tf.placeholder(tf.float32,[None, n_squares, side_square, side_square, n_channels])\n",
    "real_split_tiled = tf.tile(real_split, [samples_per_num, 1, 1, 1, 1])\n",
    "\n",
    "stack_real_images_split = tf.placeholder(tf.float32,[None, side_square, side_square, n_channels])\n",
    "\n",
    "real_entire = tf.placeholder(tf.float32,[None, side_real, side_real, n_channels])\n",
    "real_entire_tiled = tf.tile(real_entire,[samples_per_num,1,1,1])\n",
    "\n",
    "temperature = tf.constant(temp, dtype=tf.float32)\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gonzalomena/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fee58037950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fee58037950>>: AttributeError: 'module' object has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fee58037950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fee58037950>>: AttributeError: 'module' object has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fee5094bed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fee5094bed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fee5094bed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fee5094bed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fee5006e950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fee5006e950>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fee5006e950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fee5006e950>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7feeb1205cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7feeb1205cd0>>: AttributeError: 'module' object has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7feeb1205cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7feeb1205cd0>>: AttributeError: 'module' object has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fee5094bed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fee5094bed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fee5094bed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fee5094bed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fee5006e950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fee5006e950>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fee5006e950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fee5006e950>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feeb3538590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feeb3538590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feeb3538590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7feeb3538590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /Users/gonzalomena/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:2403: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From optimizer.py:49: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scrambled_split_tiled = tf.reshape(scrambled_split_tiled, [-1, n_squares, side_square ** 2* n_channels])\n",
    "real_split_tiled = tf.reshape(real_split_tiled, [-1, n_squares, side_square ** 2* n_channels])\n",
    "\n",
    "#ordered_inf = tf.matmul(inv_soft_perms_flat, scrambled_split_tiled)\n",
    "\n",
    "z,z_mean, z_log_var, cost= create_z()\n",
    "(soft_perms_inf2, _) = sinkhorn_ops.gumbel_sinkhorn(-cost, temp2, samples_per_num, noise_factor, n_iter_sinkhorn2, squeeze=False)\n",
    "inv_soft_perms2 = tf.transpose(soft_perms_inf2, [0, 1, 3, 2])\n",
    "inv_soft_perms2_flat = tf.reshape(tf.transpose(inv_soft_perms2, [1, 0, 2, 3]), [-1, n_squares, n_squares])\n",
    "\n",
    "ordered_inf2 = tf.matmul(inv_soft_perms2_flat, scrambled_split_tiled)\n",
    " \n",
    "#rec_loss =tf.reduce_mean(tf.reduce_sum(tf.square(ordered_inf2 - real_split_tiled), axis=1))\n",
    "rec_loss =-tf.reduce_mean(tf.reduce_mean(real_split_tiled*tf.log(1e-5+ordered_inf2) +(1-real_split_tiled)*tf.log(1e-5+1-ordered_inf2),axis=0))*side_real**2\n",
    "\n",
    "KL = 0.5 *tf.reduce_mean(tf.reduce_sum(1+z_log_var -tf.square(z_mean) -tf.exp(z_log_var), axis=1))\n",
    "l2s_diff = rec_loss - KL\n",
    "\n",
    "#l2s_diff = tf.reduce_mean(tf.square(ordered_inf2 - real_split_tiled))\n",
    "\n",
    "opt = optimizer.set_optimizer(opt, lr, opt_eps=1e-8)\n",
    "train_op = tf.contrib.training.create_train_op(l2s_diff, opt, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gonzalomena/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "[<tf.Variable 'conv_1/weights:0' shape=(3, 3, 1, 2) dtype=float32_ref>, <tf.Variable 'conv_1/biases:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'fully_connected/weights:0' shape=(200, 5) dtype=float32_ref>, <tf.Variable 'fully_connected/biases:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'fully_connected_1/weights:0' shape=(200, 5) dtype=float32_ref>, <tf.Variable 'fully_connected_1/biases:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'fully_connected_2/weights:0' shape=(5, 3125) dtype=float32_ref>, <tf.Variable 'fully_connected_2/biases:0' shape=(3125,) dtype=float32_ref>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gonzalomena/anaconda3/envs/python2/lib/python2.7/site-packages/ipykernel_launcher.py:105: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 335.17712]\n",
      "[10, 307.69846]\n",
      "[20, 264.22568]\n",
      "[30, 246.19595]\n",
      "[40, 237.69775]\n",
      "[50, 228.34622]\n",
      "[60, 226.39983]\n",
      "[70, 217.25107]\n",
      "[80, 212.58037]\n",
      "[90, 210.14815]\n",
      "[100, 210.71317]\n",
      "[110, 205.69243]\n",
      "[120, 212.16258]\n",
      "[130, 199.97272]\n",
      "[140, 198.53246]\n",
      "[150, 199.04738]\n",
      "[160, 199.87814]\n",
      "[170, 199.50444]\n",
      "[180, 200.60257]\n",
      "[190, 198.86267]\n",
      "[200, 194.69077]\n",
      "[210, 196.69019]\n",
      "[220, 193.01231]\n",
      "[230, 199.15096]\n",
      "[240, 192.988]\n",
      "[250, 195.44385]\n",
      "[260, 194.89201]\n",
      "[270, 196.179]\n",
      "[280, 195.34343]\n",
      "[290, 196.10367]\n",
      "[300, 193.63805]\n",
      "[310, 191.66522]\n",
      "[320, 197.21338]\n",
      "[330, 193.6347]\n",
      "[340, 192.5214]\n",
      "[350, 192.2863]\n",
      "[360, 195.92468]\n",
      "[370, 190.5558]\n",
      "[380, 193.32867]\n",
      "[390, 193.8088]\n",
      "[400, 193.344]\n",
      "[410, 189.60028]\n",
      "[420, 191.35141]\n",
      "[430, 190.10422]\n",
      "[440, 189.8514]\n",
      "[450, 186.95856]\n",
      "[460, 191.52618]\n",
      "[470, 188.70027]\n",
      "[480, 194.46085]\n",
      "[490, 185.39581]\n",
      "[500, 194.54109]\n",
      "[510, 191.26395]\n",
      "[520, 191.38763]\n",
      "[530, 189.80862]\n",
      "[540, 189.70602]\n",
      "[550, 189.49507]\n",
      "[560, 190.62762]\n",
      "[570, 189.70602]\n",
      "[580, 186.15944]\n",
      "[590, 185.86218]\n",
      "[600, 193.06564]\n",
      "[610, 188.37079]\n",
      "[620, 188.26059]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init_op=tf.initialize_all_variables()\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(init_op)\n",
    "new = tf.trainable_variables()\n",
    "print(new)\n",
    "n_iter = 2000\n",
    "for i in range(n_iter):\n",
    "    if i<n_iter-1:\n",
    "        ims,_=data_train.next_batch(batch_size)\n",
    "    else:\n",
    "        print('hola')\n",
    "        ims,_=data_test.next_batch(batch_size)        \n",
    "    ims = np.expand_dims(np.reshape(ims, [-1, 28,28]),axis=3)\n",
    "    ims[ims>0.5]=1\n",
    "    ims[ims<0.5]=0\n",
    "    np_x = resize_batch_color(ims, side, n_channels)\n",
    "\n",
    "    real_images_split = batch_split(np_x, n_squares_side, n_channels)\n",
    "    stacked_real_images_split = stack_batch_split(real_images_split)\n",
    "\n",
    "    [o2,loss, _, sp2,cos] = sess.run([ordered_inf2, l2s_diff, train_op, soft_perms_inf2,cost],{real_split:real_images_split,\n",
    "                                 scrambled_split:scrambled_pieces_split,\n",
    "                                 stack_scrambled_pieces_split:stacked_scrambled_pieces_split,\n",
    "                                                                                stack_real_images_split:stacked_real_images_split,\n",
    "                                                                               real_entire:ims})\n",
    "    if(i%10==0):\n",
    "        print([i, loss])\n",
    "    if i==n_iter-1:\n",
    "        ogen = []\n",
    "        sgen =[]\n",
    "        for k in range(5):\n",
    "            [oo2,ssp2] = sess.run([ordered_inf2,soft_perms_inf2],{z:np.random.normal(0,1,(batch_size, n_dim_z)),\n",
    "                                         scrambled_split:scrambled_pieces_split,\n",
    "                                         stack_scrambled_pieces_split:stacked_scrambled_pieces_split})\n",
    "            ogen.append(oo2)\n",
    "            sgen.append(ssp2)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_display = 10\n",
    "hard = np.zeros(sp2.shape)\n",
    "ind=3\n",
    "for i in range(batch_size_display):\n",
    "    hard[i,0,:,:] =soft_to_hard(np.transpose(sgen[ind][i,0,:,:]))\n",
    "unscrambled_images_hard =  np.matmul(hard[:batch_size_display,0,:,:], np.reshape(scrambled_pieces_split[:batch_size_display,:,:,:], [batch_size_display,n_squares_side **2 ,-1]))\n",
    "unscrambled_images_soft =  ogen[ind]\n",
    "unflatten_inf_hard = unflatten_batch(unscrambled_images_hard, n_channels)\n",
    "joined_inf_hard = join_batch_split(unflatten_inf_hard)\n",
    "unflatten_inf_soft = unflatten_batch(unscrambled_images_soft, n_channels)\n",
    "joined_inf_soft = join_batch_split(unflatten_inf_soft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(10,3,figsize=(10,30))\n",
    "#plt.imshow(joined_inf[0,:,:,0])\n",
    "print(np_x.shape)\n",
    "for i in range(10):\n",
    "    \n",
    "    ax[i,0].imshow(np_x[i,:,:,0],'gray')\n",
    "    ax[i,1].imshow(joined_inf_hard[i,:,:,0],'gray')\n",
    "    ax[i,2].imshow(joined_inf_soft[i,:,:,0],'gray')\n",
    "    for k in range(3):\n",
    "        ax[i,k].get_xaxis().set_visible(False)\n",
    "        ax[i,k].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(ssp2[0,0,:,:], axis=1))\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(ssp2[0,0,:,:])\n",
    "print(ssp2[0,0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define the main TF variables\n",
    "print(nx)\n",
    "\n",
    "# data_pieces = tf.reshape(stack_scrambled_pieces_split, [batch_size, n_squares, n_dim])\n",
    "# data_real = tf.reshape(stack_real_images_split, [batch_size, n_squares, n_dim])\n",
    "\n",
    "\n",
    "# sq = tf.reduce_sum(data_pieces **2, axis=2, keepdims=True)\n",
    "# A = tf.tile(sq, [1, 1, n_squares])\n",
    "# B = tf.tile(tf.transpose(sq, [0,2 ,1]), [1, n_squares, 1])\n",
    "# C = -2*tf.matmul(data_pieces, tf.transpose(data_pieces, [0, 2, 1]))\n",
    "# s1 = A+B +C\n",
    "\n",
    "# sq2 = tf.reduce_sum(data_real **2, axis=2, keepdims=True)\n",
    "# A2 = tf.tile(sq2, [1, 1, n_squares])\n",
    "# B2 = tf.tile(tf.transpose(sq2, [0,2 ,1]), [1, n_squares, 1])\n",
    "# C2 = -2*tf.matmul(data_real, tf.transpose(data_real, [0, 2, 1]))\n",
    "# s2 = A2+B2 +C2\n",
    "\n",
    "\n",
    "# f1 = tf.tile(tf.reduce_sum(s1**2, axis=2, keepdims=True),[1, 1, n_squares]) \n",
    "# f2 = tf.transpose(tf.tile(tf.reduce_sum(s2**2, axis=2, keepdims=True),[1, 1, n_squares]), [0, 2 ,1]) \n",
    "# soft_perms_inf = tf.cast(tf.tile(tf.constant(np.eye(n_squares))[np.newaxis,:,:],[batch_size, 1, 1]), tf.float32)\n",
    "  \n",
    "    \n",
    "# for _ in range(n_gromov):\n",
    "#     P = tf.reshape(soft_perms_inf, [-1, n_squares, n_squares])      \n",
    "#     ma = -1*(f1+f2 - 2*tf.matmul(s1, tf.matmul(P, s2)))\n",
    "#     (soft_perms_inf, _) = sinkhorn_ops.gumbel_sinkhorn(ma, temp, samples_per_num,\n",
    "#                                                                        noise_factor, n_iter_sinkhorn, squeeze=False)\n",
    "\n",
    "#inv_soft_perms = tf.transpose(soft_perms_inf, [0, 1, 3, 2])\n",
    "#inv_soft_perms_flat = tf.reshape( tf.transpose(inv_soft_perms, [1, 0, 2, 3]), [-1, n_squares, n_squares])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ims0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "91.0/(28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nx = np.nansum(ims0) \n",
    "prop = nx/(side**2)\n",
    "print(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ims0[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(n_squares*prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
