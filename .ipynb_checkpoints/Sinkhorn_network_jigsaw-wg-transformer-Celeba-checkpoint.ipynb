{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import optimizer\n",
    "import sinkhorn_ops\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from tensorflow import keras\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some ad-hoc merging, splitting and mixing functions\n",
    "\n",
    "\n",
    "def batch_split(batch, n_squares_side, n_channels=1):\n",
    "    if(n_channels ==1):\n",
    "        side = int(np.sqrt(batch.shape[1]))\n",
    "    else:\n",
    "        side = batch.shape[1]\n",
    "    batch_size = batch.shape[0]\n",
    "    n_squares = n_squares_side ** 2\n",
    "    \n",
    "    batch = np.reshape(batch, [-1, side, side, n_channels])\n",
    "    batch = np.reshape(batch, [batch_size, n_squares_side, side/n_squares_side, side, n_channels])\n",
    "    batch = np.transpose(batch, [0, 2, 1, 3, 4])\n",
    "    batch = np.reshape(batch, [batch_size, side/n_squares_side, n_squares, side/n_squares_side, n_channels])\n",
    "    batch = np.transpose(batch, [0, 2, 1, 3, 4])\n",
    "    return batch\n",
    "\n",
    "def stack_batch_split(batch):\n",
    "    return np.reshape(batch, [batch.shape[0]*batch.shape[1], batch.shape[2], batch.shape[3], batch.shape[4]])\n",
    "\n",
    "\n",
    "def unflatten_batch(batch, n_channels=1):\n",
    "    print(np.sqrt(batch.shape[2]/n_channels))\n",
    "    side_square = int(np.sqrt(batch.shape[2]/n_channels))\n",
    "    return np.reshape(batch, [batch.shape[0], batch.shape[1], side_square, side_square, n_channels])\n",
    "\n",
    "def join_batch_split(batch):\n",
    "    batch_size = batch.shape[0]\n",
    "    n_squares = batch.shape[1]\n",
    "    side_quare = batch.shape[2]\n",
    "    n_channels = batch.shape[4]\n",
    "    n_squares_side = int(np.sqrt(n_squares))\n",
    "    batch = np.transpose(batch, [0, 1, 3, 2, 4])\n",
    "    batch = np.reshape(batch, [batch_size, n_squares_side, side_square*n_squares_side, side_square, n_channels])\n",
    "    batch = np.transpose(batch, [0,1, 3,2,4])\n",
    "    batch = np.reshape(batch, [batch_size, 1, side_square*n_squares_side, side_square*n_squares_side, n_channels])\n",
    "    batch = np.reshape(batch, [batch_size, side_square*n_squares_side, side_square*n_squares_side, n_channels])\n",
    "    return batch\n",
    "\n",
    "def resized_dims(n_squares_side):\n",
    "    if(n_squares_side==2):\n",
    "        side = 28\n",
    "        side_square = 14\n",
    "    if(n_squares_side==3):\n",
    "        side = 27\n",
    "        side_square = 9\n",
    "    if(n_squares_side==4):\n",
    "        side = 28\n",
    "        side_square = 7\n",
    "    if(n_squares_side==5):\n",
    "        side = 30\n",
    "        side_square = 6\n",
    "    if(n_squares_side==6):\n",
    "        side = 30\n",
    "        side_square = 5\n",
    "    if(n_squares_side==7):\n",
    "        side = 28\n",
    "        side_square = 4\n",
    "    if(n_squares_side==8):\n",
    "        side = 32\n",
    "        side_square = 4\n",
    "    if(n_squares_side==9):\n",
    "        side = 27\n",
    "        side_square = 3\n",
    "    if(n_squares_side==16):\n",
    "        side = 32\n",
    "        side_square = 2\n",
    "    if(n_squares_side==14):\n",
    "        side = 28\n",
    "        side_square = 2\n",
    "    return side, side_square\n",
    "\n",
    "def resize_batch_color(batch, side_new, n_channels):\n",
    "    batch_new = np.zeros((batch.shape[0], side_new, side_new, n_channels))\n",
    "    side = int(np.sqrt(batch.shape[1]))\n",
    "    for i in range(batch.shape[0]):\n",
    "        for c in range(n_channels):\n",
    "            a = imresize(batch[i,:,:,c], [side_new, side_new])\n",
    "            \n",
    "            a = a/255.0\n",
    "            batch_new[i,:,:,c] =a\n",
    "    return batch_new\n",
    "\n",
    "\n",
    "def soft_to_hard(soft_perm):\n",
    "    \n",
    "    a,b = linear_sum_assignment(-soft_perm)\n",
    "    ma = np.zeros((np.shape(soft_perm)))\n",
    "    for i in range(soft_perm.shape[0]):\n",
    "        ma[i, b[i]] = 1\n",
    "    return ma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model params\n",
    "batch_size = 50\n",
    "n_iter =50\n",
    "samples_per_num = 1\n",
    "n_iter_sinkhorn = 20\n",
    "noise_factor = 1.0\n",
    "keep_prob =1.0\n",
    "opt = 'sgd'\n",
    "n_units = 2\n",
    "temp = 0.25\n",
    "lr = 0.00001\n",
    "\n",
    "#mnist data\n",
    "rfield_size = 3\n",
    "n_squares_side = 14\n",
    "n_channels = 3\n",
    "stride = 2\n",
    "n_squares = n_squares_side **2\n",
    "n_dim = 10\n",
    "side, side_square = resized_dims(n_squares_side)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main neural network definitions\n",
    "\n",
    "def create_log_alpha():\n",
    "    #create the matrix of log_alpha, that will later will converted into a soft permutation\n",
    "    #this relies on some NN processing (convolutional), see below\n",
    "    fc = tf.contrib.layers.fully_connected\n",
    "    flatten = tf.contrib.layers.flatten\n",
    "    dropout = tf.contrib.layers.dropout\n",
    "    def conv(input_image, kernel_shape, bias_shape):\n",
    "        weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "                                 initializer = tf.random_normal_initializer())\n",
    "        biases = tf.get_variable(\"biases\", bias_shape, \n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "        convolutional = tf.nn.conv2d(input_image, weights, \n",
    "                                     strides = [1, 1, 1, 1],\n",
    "                                     padding=\"SAME\")\n",
    "        out_relu = tf.nn.relu(convolutional + biases)\n",
    "        out_maxpool = tf.nn.max_pool(out_relu, \n",
    "                                    ksize=[1, stride, stride, 1],\n",
    "                                   strides=[1, stride, stride, 1],\n",
    "                                   padding=\"SAME\")\n",
    "        return out_maxpool\n",
    "    def conv_and_fc(input_image):\n",
    "        with tf.variable_scope(\"conv1\"):\n",
    "            conv_output = conv(input_image, [rfield_size, rfield_size, n_channels, n_units], [n_units])\n",
    "        fully_connected_output = dropout(tf.cast(fc(flatten(conv_output), n_dim, activation_fn = None), tf.float32),\n",
    "                                        keep_prob)\n",
    "        return fully_connected_output\n",
    "    \n",
    "    with tf.variable_scope(\"model_params\"):\n",
    "        log_alpha = tf.nn.sigmoid(tf.reshape(conv_and_fc(stack_scrambled_images_split), [-1, n_squares, n_dim]))\n",
    "        #log_alpha = tf.reshape(conv_and_fc(stack_scrambled_images_split), [-1, n_squares, n_dim])\n",
    "        \n",
    "        sq = tf.reduce_sum(log_alpha **2, axis=2, keepdims=True)\n",
    "        A = tf.tile(sq, [1, 1, n_squares])\n",
    "        B = tf.tile(tf.transpose(sq, [0,2 ,1]), [1, n_squares, 1])\n",
    "        C = -2*tf.matmul(log_alpha, tf.transpose(log_alpha, [0, 2, 1]))\n",
    "        s1 = A+B +C\n",
    "    with tf.variable_scope(\"model_params\", reuse=True):    \n",
    "        log_alpha2 = tf.nn.sigmoid(tf.reshape(conv_and_fc(stack_real_images_split), [-1, n_squares, n_dim]))\n",
    "        #log_alpha2 = tf.reshape(conv_and_fc(stack_real_images_split), [-1, n_squares, n_dim])\n",
    "        sq2 = tf.reduce_sum(log_alpha2 **2, axis=2, keepdims=True)\n",
    "        A2 = tf.tile(sq2, [1, 1, n_squares])\n",
    "        B2 = tf.tile(tf.transpose(sq2, [0,2 ,1]), [1, n_squares, 1])\n",
    "        C2 = tf.matmul(-2*log_alpha2, tf.transpose(log_alpha2, [0, 2, 1]))\n",
    "        s2 = A2+B2 +C2\n",
    "        \n",
    "    return s1,s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gomena/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/gomena/.local/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/gomena/.local/lib/python2.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/gomena/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "#Now we define the main TF variables\n",
    "\n",
    "scrambled_split = tf.placeholder(tf.float32,[None, n_squares, side_square, side_square, n_channels])\n",
    "scrambled_split_tiled = tf.tile(scrambled_split, [samples_per_num, 1, 1, 1, 1])\n",
    "\n",
    "stack_scrambled_images_split = tf.placeholder(tf.float32,[None, side_square, side_square, n_channels])\n",
    "\n",
    "real_split = tf.placeholder(tf.float32,[None, n_squares, side_square, side_square, n_channels])\n",
    "real_split_tiled = tf.tile(real_split, [samples_per_num, 1, 1, 1, 1])\n",
    "\n",
    "stack_real_images_split = tf.placeholder(tf.float32,[None, side_square, side_square, n_channels])\n",
    "\n",
    "\n",
    "temperature = tf.constant(temp, dtype=tf.float32)\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "fc = tf.contrib.layers.fully_connected\n",
    "\n",
    "s1,s2 = create_log_alpha()\n",
    "f1 = tf.tile(tf.reduce_sum(s1**2, axis=2, keepdims=True),[1, 1, n_squares]) \n",
    "f2 = tf.transpose(tf.tile(tf.reduce_sum(s2**2, axis=2, keepdims=True),[1, 1, n_squares]), [0, 2 ,1]) \n",
    "soft_perms_inf = tf.cast(tf.tile(tf.constant(np.eye(n_squares))[np.newaxis,:,:],[batch_size, 1, 1]), tf.float32)\n",
    "          \n",
    "for _ in range(3):\n",
    "    P = tf.reshape(soft_perms_inf, [-1, n_squares, n_squares])      \n",
    "    ma = -1*(f1+f2 - 2*tf.matmul(s1, tf.matmul(P, s2)))\n",
    "    (soft_perms_inf, _) = sinkhorn_ops.gumbel_sinkhorn(ma, temp, samples_per_num,\n",
    "                                                                       noise_factor, n_iter_sinkhorn, squeeze=False)\n",
    "\n",
    "P = tf.reshape(soft_perms_inf, [-1, n_squares, n_squares])\n",
    "l2s_diff = tf.reduce_sum(tf.matmul(P, -ma))   \n",
    "\n",
    "soft_perms_inf1 = tf.cast(tf.tile(tf.constant(np.eye(n_squares))[np.newaxis,:,:],[batch_size, 1, 1]), tf.float32)\n",
    "P1 = tf.reshape(soft_perms_inf1, [-1, n_squares, n_squares])      \n",
    "ma1= -1*(f1+f2 - 2*tf.matmul(s1, tf.matmul(P1, s2)))\n",
    "(soft_perms_inf1, _) = sinkhorn_ops.gumbel_sinkhorn(ma1, temp, samples_per_num,\n",
    "                                                                   noise_factor, n_iter_sinkhorn, squeeze=False)\n",
    "\n",
    "\n",
    "\n",
    "inv_soft_perms = tf.transpose(soft_perms_inf, [0, 1, 3, 2])\n",
    "inv_soft_perms_flat = tf.reshape( tf.transpose(inv_soft_perms, [1, 0, 2, 3]), [-1, n_squares, n_squares])\n",
    "\n",
    "real_split_tiled = tf.reshape(real_split_tiled, [-1, n_squares, side_square ** 2 * n_channels])\n",
    "scrambled_split_tiled = tf.reshape(scrambled_split_tiled, [-1, n_squares, side_square ** 2* n_channels])\n",
    "ordered_inf = tf.matmul(inv_soft_perms_flat, scrambled_split_tiled)\n",
    "l2s_diff = tf.reduce_mean(tf.square(real_split_tiled - ordered_inf))\n",
    "opt = optimizer.set_optimizer(opt, lr, opt_eps=1e-8)\n",
    "train_op = tf.contrib.training.create_train_op(l2s_diff, opt, global_step=global_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gomena/.local/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gomena/.local/lib/python2.7/site-packages/ipykernel_launcher.py:79: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: 0.1033634\n"
     ]
    }
   ],
   "source": [
    "#Lets train the model\n",
    "train,test= tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "init_op=tf.initialize_all_variables()\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(init_op)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    batch = np.random.choice(train[0].shape[0],batch_size,replace=False)\n",
    "    np_x = train[0][batch,:,:,:]\n",
    "    np_x = resize_batch_color(np_x, side, n_channels)\n",
    "   \n",
    "    batch = np.random.choice(train[0].shape[0],batch_size,replace=False)\n",
    "    np_x2 = train[0][batch,:,:,:]\n",
    "    np_x2 = resize_batch_color(np_x2, side, n_channels)\n",
    "   \n",
    "    real_images_split = batch_split(np_x, n_squares_side, n_channels)\n",
    "    real_images_split2 = batch_split(np_x2, n_squares_side, n_channels)\n",
    "    \n",
    "    scrambled_images_split = np.zeros(real_images_split.shape)\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        perm = np.random.permutation(n_squares)\n",
    "        scrambled_images_split[j,:, :, :] = real_images_split2[j, perm, :, :]\n",
    "    stacked_scrambled_images_split = stack_batch_split(scrambled_images_split)\n",
    "    stacked_real_images_split = stack_batch_split(real_images_split)\n",
    "    _,loss=sess.run([train_op,l2s_diff],{real_split:real_images_split,\n",
    "                                         scrambled_split:scrambled_images_split,\n",
    "                                        stack_scrambled_images_split:stacked_scrambled_images_split,\n",
    "                                        stack_real_images_split:stacked_real_images_split})\n",
    "    \n",
    "   \n",
    "    if i % 50 == 1:\n",
    "        print('Step %d, Loss: %0.7f' % (i,loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now let's test the model\n",
    "#np.random.seed(0)\n",
    "#tf.random.set_random_seed(0)\n",
    "batch_size_test=batch_size\n",
    "batch = np.random.choice(test[0].shape[0],batch_size,replace=False)\n",
    "np_x = test[0][batch,:,:,:]\n",
    "np_x = resize_batch_color(np_x, side, n_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = np.random.choice(test[0].shape[0],batch_size,replace=False)\n",
    "np_x2 = test[0][batch,:,:,:]\n",
    "np_x2 = resize_batch_color(np_x2, side, n_channels)\n",
    "\n",
    "real_images_split = batch_split(np_x, n_squares_side, n_channels)\n",
    "#p = 0.2\n",
    "#real_images_split = np.tile(np.random.binomial(1,p,[50,196,1,1,1]), [1,1,2,2,1])\n",
    "real_images_split2 = batch_split(np_x2, n_squares_side, n_channels)\n",
    "\n",
    "scrambled_images_split = np.zeros(real_images_split.shape)\n",
    "    \n",
    "for i in range(batch_size_test):\n",
    "    perm = np.random.permutation(n_squares)\n",
    "    #perm = np.arange(n_squares)\n",
    "    scrambled_images_split[i,:, :, :] = real_images_split[i, perm, :, :]\n",
    "stacked_scrambled_images_split = stack_batch_split(scrambled_images_split)\n",
    "stacked_real_images_split = stack_batch_split(real_images_split2)\n",
    "[unscrambled_images,loss, inv_soft_perms_np,ma_np,s1_np,s2_np,soft_inv2] = sess.run([ordered_inf,\n",
    "                                                                           l2s_diff,\n",
    "                                                                           inv_soft_perms,\n",
    "                                                                           ma,s1,s2,soft_perms_inf1],\n",
    "                                                                          {real_split:real_images_split2,\n",
    "                            scrambled_split:scrambled_images_split,\n",
    "                            stack_scrambled_images_split:stacked_scrambled_images_split,\n",
    "                                                                           stack_real_images_split:stacked_real_images_split})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_display = 5\n",
    "unscrambled_images = unscrambled_images[:batch_size_display,:,:]\n",
    "inv_hard_perms= np.zeros((batch_size_display, n_squares, n_squares))\n",
    "for i in range(batch_size_display):\n",
    "    inv_hard_perms[i,:,:] = soft_to_hard(inv_soft_perms_np[i,0,:,:])\n",
    "    #inv_hard_perms[i,:,:] = soft_inv2[i,0,:,:].T\n",
    "unflatten_inf = unflatten_batch(unscrambled_images, n_channels)\n",
    "hard_inf = np.matmul(inv_hard_perms, np.reshape(scrambled_images_split[:batch_size_display,:,:,:], [batch_size_display,n_squares_side **2 ,-1]))\n",
    "\n",
    "unflatten_hard_inf =np.reshape(hard_inf, unflatten_inf.shape)\n",
    "joined_hard_inf =join_batch_split(unflatten_hard_inf)\n",
    "joined_inf = join_batch_split(unflatten_inf)\n",
    "joined_scrambled = join_batch_split(scrambled_images_split)\n",
    "joined_real = join_batch_split(real_images_split2)\n",
    "\n",
    "#Compare reconstructions with real data\n",
    "\n",
    "fig, ax = plt.subplots(batch_size_display,4,figsize=(10,14))\n",
    "\n",
    "for i in range(batch_size_display):\n",
    "   \n",
    "    ax[i,0].imshow(joined_real[i,:,:,:],cmap='Greys')\n",
    "    ax[i,0].get_xaxis().set_visible(False)\n",
    "    ax[i,0].get_yaxis().set_visible(False)\n",
    "    \n",
    "    ax[i,1].imshow(joined_scrambled[i,:,:,:],cmap='Greys')\n",
    "    ax[i,1].get_xaxis().set_visible(False)\n",
    "    ax[i,1].get_yaxis().set_visible(False)\n",
    "    \n",
    "    ax[i,2].imshow(joined_inf[i,:,:,:],cmap='Greys')\n",
    "    ax[i,2].get_xaxis().set_visible(False)\n",
    "    ax[i,2].get_yaxis().set_visible(False)\n",
    "     \n",
    "    ax[i,3].imshow(joined_hard_inf[i,:,:,:],cmap='Greys')\n",
    "    ax[i,3].get_xaxis().set_visible(False)\n",
    "    ax[i,3].get_yaxis().set_visible(False)\n",
    "   \n",
    "\n",
    "    if(i==0):\n",
    "        ax[i,0].set_title('Real',fontsize =25)\n",
    "        ax[i,1].set_title('Real Scrambled',fontsize =25)\n",
    "        ax[i,2].set_title('Soft Reconstructed',fontsize =25)\n",
    "        ax[i,3].set_title('Hard Reconstructed',fontsize =25)\n",
    "plt.savefig('reconstruct2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].imshow(inv_soft_perms_np[0,0,:,:])\n",
    "ax[1].imshow(inv_hard_perms[0,:,:])\n",
    "print(inv_soft_perms_np.shape)\n",
    "print(inv_soft_perms_np[0,0,0,:])\n",
    "print(inv_hard_perms[0,0,:])\n",
    "print(np.nansum(inv_soft_perms_np[0,0,:,:],axis=0))\n",
    "print(inv_soft_perms_np[0,0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].imshow(s1_np[2,:,:])\n",
    "ax[1].imshow(s2_np[2,:,:])\n",
    "print(real_images_split.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(real_images_split.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.binomial(np.random.binomial(1,p,[50,196,1,1,1]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
